{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "815e61a3-5ac6-4d98-bb1a-cf00f3020871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c4f84-81aa-4137-ab4c-23c65d735e84",
   "metadata": {},
   "source": [
    "## Getting Images for Training and Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7060a59-43b0-49a6-9f3d-87727ae4aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('cat_v1/'):\n",
    "    for filename in filenames:\n",
    "        breed = os.path.basename(dirname)  # Extract breed from the directory\n",
    "        filepath = os.path.join(dirname, filename)\n",
    "        data.append({'filename': filename, 'breed': breed})\n",
    "cwd = os.getcwd()\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1371bfd-bb2c-42ab-ac88-a637e07040d6",
   "metadata": {},
   "source": [
    "## Resizing Images for Faster Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bfc4c4-0865-44c6-9ca4-5e036261efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file  cat_v1/maine_coon/2003-4288-2848-dsc-8088-2e700.dsc-8088.htm  can not be processed by cv2\n",
      "Resizing images took  18.913495779037476  seconds\n"
     ]
    }
   ],
   "source": [
    "# Code from user gpiosenka on Kaggle\n",
    "\n",
    "start = time.time()\n",
    "height = 200\n",
    "width = 224\n",
    "working_dir = r''\n",
    "resized_dir = os.path.join(working_dir, 'resized')\n",
    "os.makedirs(resized_dir, exist_ok=True)  # Create resized_dir if it doesn't exist\n",
    "source_dir = r'cat_v1'\n",
    "classes = [klass for klass in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, klass))]\n",
    "\n",
    "for klass in classes:\n",
    "    msg = f'Resizing images for class {klass}                                             '\n",
    "    print(msg, '\\r', end='')    \n",
    "    classpath = os.path.join(source_dir, klass)\n",
    "    dest_classpath = os.path.join(resized_dir, klass)\n",
    "    os.makedirs(dest_classpath, exist_ok=True)  # Create class directories in the resized directory\n",
    "    \n",
    "    flist = [f for f in os.listdir(classpath) if os.path.isfile(os.path.join(classpath, f))]\n",
    "\n",
    "    for f in flist:\n",
    "        fpath = os.path.join(classpath, f)\n",
    "        dest_fpath = os.path.join(dest_classpath, f)\n",
    "        try:\n",
    "            img = cv2.imread(fpath)        \n",
    "            img = cv2.resize(img, (height, width))\n",
    "            cv2.imwrite(dest_fpath, img)  # Save the resized image\n",
    "        except:\n",
    "            print('file ', fpath, ' can not be processed by cv2')\n",
    "\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print('Resizing images took ', duration, ' seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2799d-0af6-4653-a861-7fdfe4e51892",
   "metadata": {},
   "source": [
    "## Image Data Setup for Keras CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d8040743-43f2-4c50-a469-4a7bb1243997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 952 files belonging to 5 classes.\n",
      "Using 762 files for training.\n",
      "Found 952 files belonging to 5 classes.\n",
      "Using 190 files for validation.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"resized/\"\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "resized_dims = (200,224)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=resized_dims, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=resized_dims,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca6fd8-bcd4-4185-84d2-0cf3cc732979",
   "metadata": {},
   "source": [
    "## Build Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "121f3ae6-9140-4f0a-ad46-cc5a3b4805bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "# copied from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f93c453c-dfc3-43bd-87df-5287f53b8c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created EfficientNetB0 model with initial learning rate set to 0.001\n"
     ]
    }
   ],
   "source": [
    "img_shape = (200,224,3)\n",
    "#we resized our images to 200x224, images are RGB, so 3 colors\n",
    "\n",
    "class_count = len(df['breed'].unique()) #5\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=img_shape)\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=inputs, pooling='max')\n",
    "\n",
    "# Additional layers\n",
    "x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(base_model.output)\n",
    "x = Dense(256, kernel_regularizer=regularizers.l2(l=0.016),\n",
    "          activity_regularizer=regularizers.l1(0.006),\n",
    "          bias_regularizer=regularizers.l1(0.006), activation='relu')(x)\n",
    "x = Dropout(rate=0.4, seed=123)(x)\n",
    "output = Dense(class_count, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "lr = 0.001\n",
    "model.compile(optimizer=Adamax(learning_rate=lr), #using legacy adamax because of keras warning \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy', F1_score])\n",
    "\n",
    "print(f'Created EfficientNetB0 model with initial learning rate set to {lr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bbb5ba-a16f-46ac-aef3-f554980c5a9e",
   "metadata": {},
   "source": [
    "## Setup Model & Callbacks for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "659d8a9e-45c9-4dd8-bc7f-960798219bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "cwd = os.getcwd()\n",
    "checkpoint_filepath = f'{cwd}/checkpoint.model.keras'\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "steps_per_epoch = train_ds.cardinality().numpy()\n",
    "validation_steps = val_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8439a7a-1749-45a6-b471-058a1568afcc",
   "metadata": {},
   "source": [
    "## Model Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9e280a3d-6aac-4733-ba64-e532b498e42a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/48\n",
      "24/24 [==============================] - 40s 2s/step - loss: 5.9224 - accuracy: 0.9764 - F1_score: 0.9826 - val_loss: 6.3364 - val_accuracy: 0.8158 - val_F1_score: 0.9669\n",
      "Epoch 2/48\n",
      "24/24 [==============================] - 37s 2s/step - loss: 5.4995 - accuracy: 0.9724 - F1_score: 0.9772 - val_loss: 5.8746 - val_accuracy: 0.8000 - val_F1_score: 1.0060\n",
      "Epoch 3/48\n",
      "24/24 [==============================] - 32s 1s/step - loss: 5.0721 - accuracy: 0.9803 - F1_score: 0.9516 - val_loss: 5.4686 - val_accuracy: 0.8105 - val_F1_score: 0.9693\n",
      "Epoch 4/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 4.6911 - accuracy: 0.9882 - F1_score: 0.9434 - val_loss: 5.0886 - val_accuracy: 0.8263 - val_F1_score: 0.9814\n",
      "Epoch 5/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 4.3419 - accuracy: 0.9934 - F1_score: 0.9406 - val_loss: 4.7058 - val_accuracy: 0.8211 - val_F1_score: 0.9567\n",
      "Epoch 6/48\n",
      "24/24 [==============================] - 40s 2s/step - loss: 4.0183 - accuracy: 0.9974 - F1_score: 0.9431 - val_loss: 4.4128 - val_accuracy: 0.8368 - val_F1_score: 0.9679\n",
      "Epoch 7/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 3.7187 - accuracy: 0.9987 - F1_score: 0.9320 - val_loss: 4.1153 - val_accuracy: 0.8316 - val_F1_score: 0.9635\n",
      "Epoch 8/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 3.4636 - accuracy: 0.9908 - F1_score: 0.9463 - val_loss: 3.8315 - val_accuracy: 0.8263 - val_F1_score: 1.0208\n",
      "Epoch 9/48\n",
      "24/24 [==============================] - 32s 1s/step - loss: 3.2079 - accuracy: 0.9948 - F1_score: 0.9451 - val_loss: 3.5989 - val_accuracy: 0.8368 - val_F1_score: 1.0409\n",
      "Epoch 10/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 2.9742 - accuracy: 0.9948 - F1_score: 0.9398 - val_loss: 3.4718 - val_accuracy: 0.8211 - val_F1_score: 1.0383\n",
      "Epoch 11/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 2.7705 - accuracy: 0.9882 - F1_score: 0.9327 - val_loss: 3.1624 - val_accuracy: 0.8526 - val_F1_score: 1.0269\n",
      "Epoch 12/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 2.5522 - accuracy: 0.9987 - F1_score: 0.9215 - val_loss: 2.9688 - val_accuracy: 0.8579 - val_F1_score: 1.0221\n",
      "Epoch 13/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 2.3644 - accuracy: 0.9961 - F1_score: 0.9157 - val_loss: 2.7569 - val_accuracy: 0.8474 - val_F1_score: 1.0040\n",
      "Epoch 14/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 2.2009 - accuracy: 0.9948 - F1_score: 0.9174 - val_loss: 2.6042 - val_accuracy: 0.8579 - val_F1_score: 1.0389\n",
      "Epoch 15/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 2.0388 - accuracy: 0.9987 - F1_score: 0.9266 - val_loss: 2.4096 - val_accuracy: 0.8684 - val_F1_score: 1.0260\n",
      "Epoch 16/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 1.8911 - accuracy: 0.9974 - F1_score: 0.9149 - val_loss: 2.2970 - val_accuracy: 0.8474 - val_F1_score: 1.0590\n",
      "Epoch 17/48\n",
      "24/24 [==============================] - 36s 1s/step - loss: 1.7572 - accuracy: 0.9974 - F1_score: 0.9184 - val_loss: 2.1780 - val_accuracy: 0.8421 - val_F1_score: 1.0354\n",
      "Epoch 18/48\n",
      "24/24 [==============================] - 38s 2s/step - loss: 1.6442 - accuracy: 0.9961 - F1_score: 0.9401 - val_loss: 2.0244 - val_accuracy: 0.8526 - val_F1_score: 1.0126\n",
      "Epoch 19/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 1.5201 - accuracy: 0.9948 - F1_score: 0.9125 - val_loss: 1.9013 - val_accuracy: 0.8526 - val_F1_score: 1.0273\n",
      "Epoch 20/48\n",
      "24/24 [==============================] - 32s 1s/step - loss: 1.4018 - accuracy: 0.9961 - F1_score: 0.9183 - val_loss: 1.8056 - val_accuracy: 0.8474 - val_F1_score: 1.0277\n",
      "Epoch 21/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 1.3084 - accuracy: 0.9974 - F1_score: 0.9233 - val_loss: 1.7313 - val_accuracy: 0.8526 - val_F1_score: 1.0010\n",
      "Epoch 22/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 1.2127 - accuracy: 0.9987 - F1_score: 0.9141 - val_loss: 1.6338 - val_accuracy: 0.8474 - val_F1_score: 0.9956\n",
      "Epoch 23/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 1.1273 - accuracy: 0.9961 - F1_score: 0.9085 - val_loss: 1.5477 - val_accuracy: 0.8526 - val_F1_score: 0.9973\n",
      "Epoch 24/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 1.0491 - accuracy: 0.9987 - F1_score: 0.9120 - val_loss: 1.4771 - val_accuracy: 0.8474 - val_F1_score: 1.0082\n",
      "Epoch 25/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 0.9835 - accuracy: 0.9974 - F1_score: 0.9209 - val_loss: 1.4117 - val_accuracy: 0.8789 - val_F1_score: 1.0183\n",
      "Epoch 26/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 0.9076 - accuracy: 1.0000 - F1_score: 0.9081 - val_loss: 1.3252 - val_accuracy: 0.8684 - val_F1_score: 1.0127\n",
      "Epoch 27/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.8431 - accuracy: 0.9974 - F1_score: 0.9104 - val_loss: 1.2494 - val_accuracy: 0.8632 - val_F1_score: 0.9899\n",
      "Epoch 28/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.7864 - accuracy: 0.9974 - F1_score: 0.9012 - val_loss: 1.2069 - val_accuracy: 0.8632 - val_F1_score: 0.9941\n",
      "Epoch 29/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.7260 - accuracy: 0.9987 - F1_score: 0.9143 - val_loss: 1.1634 - val_accuracy: 0.8632 - val_F1_score: 0.9738\n",
      "Epoch 30/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.6824 - accuracy: 0.9987 - F1_score: 0.9103 - val_loss: 1.1116 - val_accuracy: 0.8579 - val_F1_score: 1.0106\n",
      "Epoch 31/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 0.6401 - accuracy: 0.9987 - F1_score: 0.9017 - val_loss: 1.0887 - val_accuracy: 0.8632 - val_F1_score: 1.0012\n",
      "Epoch 32/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 0.5922 - accuracy: 0.9987 - F1_score: 0.9107 - val_loss: 1.0289 - val_accuracy: 0.8684 - val_F1_score: 0.9954\n",
      "Epoch 33/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.5531 - accuracy: 1.0000 - F1_score: 0.9064 - val_loss: 0.9818 - val_accuracy: 0.8684 - val_F1_score: 0.9869\n",
      "Epoch 34/48\n",
      "24/24 [==============================] - 40s 2s/step - loss: 0.5224 - accuracy: 0.9987 - F1_score: 0.9069 - val_loss: 0.9471 - val_accuracy: 0.8632 - val_F1_score: 0.9648\n",
      "Epoch 35/48\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.4966 - accuracy: 0.9961 - F1_score: 0.9117 - val_loss: 0.9306 - val_accuracy: 0.8474 - val_F1_score: 0.9643\n",
      "Epoch 36/48\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.4637 - accuracy: 0.9961 - F1_score: 0.9051 - val_loss: 0.9138 - val_accuracy: 0.8526 - val_F1_score: 0.9598\n",
      "Epoch 37/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.4350 - accuracy: 0.9974 - F1_score: 0.9108 - val_loss: 0.8825 - val_accuracy: 0.8421 - val_F1_score: 0.9905\n",
      "Epoch 38/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.4088 - accuracy: 0.9961 - F1_score: 0.9105 - val_loss: 0.8735 - val_accuracy: 0.8474 - val_F1_score: 1.0342\n",
      "Epoch 39/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.3992 - accuracy: 0.9974 - F1_score: 0.9032 - val_loss: 0.8262 - val_accuracy: 0.8421 - val_F1_score: 1.0091\n",
      "Epoch 40/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.3744 - accuracy: 0.9987 - F1_score: 0.9045 - val_loss: 0.7966 - val_accuracy: 0.8526 - val_F1_score: 0.9875\n",
      "Epoch 41/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.3465 - accuracy: 0.9987 - F1_score: 0.9063 - val_loss: 0.7759 - val_accuracy: 0.8526 - val_F1_score: 0.9532\n",
      "Epoch 42/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 0.3394 - accuracy: 0.9961 - F1_score: 0.9079 - val_loss: 0.7650 - val_accuracy: 0.8579 - val_F1_score: 0.9463\n",
      "Epoch 43/48\n",
      "24/24 [==============================] - 33s 1s/step - loss: 0.3201 - accuracy: 0.9974 - F1_score: 0.9042 - val_loss: 0.7762 - val_accuracy: 0.8526 - val_F1_score: 0.9620\n",
      "Epoch 44/48\n",
      "24/24 [==============================] - 39s 2s/step - loss: 0.3104 - accuracy: 0.9974 - F1_score: 0.9094 - val_loss: 0.7629 - val_accuracy: 0.8684 - val_F1_score: 0.9842\n",
      "Epoch 45/48\n",
      "24/24 [==============================] - 34s 1s/step - loss: 0.2940 - accuracy: 0.9987 - F1_score: 0.9047 - val_loss: 0.7333 - val_accuracy: 0.8632 - val_F1_score: 0.9613\n",
      "Epoch 46/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.2808 - accuracy: 0.9974 - F1_score: 0.9076 - val_loss: 0.7346 - val_accuracy: 0.8526 - val_F1_score: 0.9275\n",
      "Epoch 47/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.2726 - accuracy: 0.9961 - F1_score: 0.9113 - val_loss: 0.7411 - val_accuracy: 0.8421 - val_F1_score: 0.9995\n",
      "Epoch 48/48\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.2606 - accuracy: 0.9974 - F1_score: 0.9026 - val_loss: 0.7390 - val_accuracy: 0.8474 - val_F1_score: 0.9757\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=48, steps_per_epoch=steps_per_epoch, \n",
    "                    validation_data=val_ds, validation_steps=validation_steps,\n",
    "                    callbacks=[model_checkpoint_callback, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c637acbe-a6e6-4e30-bef7-4ac9555e3b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Validation Accuracy: 0.878947377204895\n",
      "Best Epoch: 27\n"
     ]
    }
   ],
   "source": [
    "log = pd.read_csv('training.log')\n",
    "print(f'Peak Validation Accuracy: {max(log.val_accuracy)}\\nBest Epoch: {np.argmax(log.val_accuracy)+1}')\n",
    "# printed epoch output is incorrect because an issue with overwriting my training log, \n",
    "# should be epoch 25, code is writted properly and the printed output will be correct for any future runs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
